# 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

# 2ï¸âƒ£ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV (ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…Ù„Ù products.csv Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯)
csv_path = "MyDream/marketing_data.csv"
df = pd.read_csv(csv_path)
print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
print(df.head())

# 3ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù†ØµÙˆØµ ØªØ¯Ø±ÙŠØ¨ÙŠØ© (Ù…Ù† ØºÙŠØ± generated_ad)
texts = []
for _, row in df.iterrows():
    text = (
        f"Ø§Ù„Ù…Ù†ØªØ¬: {row['product_name']} | "
        f"Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù: {row['target_audience']} | "
        f"Ø§Ù„Ø¹Ù…Ø±: {row['target_age']} | "
        f"Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {row['main_benefit']} | "
        f"Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª: {row['features']} | "
        f"Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ: {row['social_proof']} | "
        f"Ø§Ù„Ù†Ø¯Ø±Ø©/Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„: {row['scarcity_urgency']} | "
        f"Ø§Ù„Ù‚ØµØ©: {row['storytelling']} | "
        f"Ø¶Ù…Ø§Ù† Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©: {row['risk_reversal']} | "
        f"Ø§Ù„Ù…Ø­ÙØ²Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©: {row['emotional_triggers']} | "
        f"Ù„ØºØ© Ù‚ÙˆÙŠØ©: {row['powerful_language']} | "
        f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ³Ø¹ÙŠØ±: {row['pricing_strategy']} | "
        f"Ø§Ù„ØµÙˆØ±: {row['visuals']} | "
        f"Ù†Ù‚Ø·Ø© Ø§Ù„Ø£Ù„Ù…: {row['pain_point']} | "
        f"Ø§Ù„Ø­Ù„: {row['solution']}"
    )
    texts.append(text)

# 4ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ù„Ù Ù†ØµÙŠ
with open("ads.txt", "w", encoding="utf-8") as f:
    for t in texts:
        f.write(t + "\n")

# 5ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
dataset = load_dataset('text', data_files={'train': 'ads.txt'})

# 6ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø¯Ø§Ø¹Ù… Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
model_name = "aubmindlab/aragpt2-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 7ï¸âƒ£ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Tokenization)
def tokenize_function(examples):
    tokens = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 8ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_args = TrainingArguments(
    output_dir="./aragpt2-ads",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_dir='./logs',
    save_strategy="epoch",
    report_to="none"
)

# 9ï¸âƒ£ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø¯Ø±Ø¨ (Trainer)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"]
)

# ğŸ”Ÿ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
trainer.train()

# 1ï¸âƒ£1ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø­Ù„ÙŠÙ‹Ø§
save_path = "./aragpt2-ads"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ­ÙØ¸Ù‡ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ø¨Ù†Ø¬Ø§Ø­!")
